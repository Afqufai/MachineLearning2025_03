{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba84c46d",
   "metadata": {},
   "source": [
    "# Praktikum 1\n",
    "Backpropagation adalah sebuah algoritma untuk melatih jaringan saraf tiruan dengan cara mengoreksi kesalahan. Algoritma ini bekerja dengan cara menghitung selisih antara keluaran yang dihasilkan jaringan dan keluaran yang seharusnya (kesalahan), lalu memperbarui bobot dan bias jaringan secara berulang dari keluaran ke masukan untuk meminimalkan kesalahan tersebut. Cara kerja backpropagation\n",
    "\n",
    "## **Langkah:**\n",
    "1. Buat dataset sederhana (XOR).\n",
    "2. Inisialisasi bobot dan bias.\n",
    "3. Implementasikan forward pass.\n",
    "4. Hitung error dan lakukan backpropagation.\n",
    "5. Update bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2619332267782331\n",
      "Epoch 1000, Loss: 0.2501244406618378\n",
      "Epoch 2000, Loss: 0.24906861385493773\n",
      "Epoch 3000, Loss: 0.2365657738759503\n",
      "Epoch 4000, Loss: 0.18907027183797684\n",
      "Epoch 5000, Loss: 0.1391599756557707\n",
      "Epoch 6000, Loss: 0.042915651887457926\n",
      "Epoch 7000, Loss: 0.015589450871598111\n",
      "Epoch 8000, Loss: 0.008556741298306412\n",
      "Epoch 9000, Loss: 0.005702193777942909\n",
      "Prediksi:\n",
      "[[0.05409118]\n",
      " [0.93820342]\n",
      " [0.93850879]\n",
      " [0.0795674 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8250cd2",
   "metadata": {},
   "source": [
    "## Tugas 1\n",
    "- Ubah jumlah neuron hidden layer menjadi 3.\n",
    "- Bandingkan hasil loss dengan konfigurasi awal.\n",
    "- Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35561761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.25647652368292245\n",
      "Epoch 1000, Loss: 0.2262062389803688\n",
      "Epoch 2000, Loss: 0.1663899477210308\n",
      "Epoch 3000, Loss: 0.144680996453321\n",
      "Epoch 4000, Loss: 0.13712708440096893\n",
      "Epoch 5000, Loss: 0.13358385583612614\n",
      "Epoch 6000, Loss: 0.13158054781656542\n",
      "Epoch 7000, Loss: 0.1303077695314678\n",
      "Epoch 8000, Loss: 0.12943330307970377\n",
      "Epoch 9000, Loss: 0.1287980484692272\n",
      "Prediksi:\n",
      "[[0.04680981]\n",
      " [0.49570315]\n",
      " [0.94936619]\n",
      " [0.50418181]]\n"
     ]
    }
   ],
   "source": [
    "# UBAH JUMLAH NEURON HIDDEN MENJADI TIGA\n",
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size_THREE = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size_THREE)\n",
    "b1 = np.zeros((1, hidden_size_THREE))\n",
    "W2 = np.random.randn(hidden_size_THREE, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a46fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.250001\n",
      "Epoch 1000, Loss: 0.250000\n",
      "Epoch 2000, Loss: 0.250000\n",
      "Epoch 3000, Loss: 0.250000\n",
      "Epoch 4000, Loss: 0.250000\n",
      "Epoch 5000, Loss: 0.250000\n",
      "Epoch 6000, Loss: 0.250000\n",
      "Epoch 7000, Loss: 0.250000\n",
      "Epoch 8000, Loss: 0.250000\n",
      "Epoch 9000, Loss: 0.250000\n",
      "\n",
      "Output Akhir (Prediksi):\n",
      "[[0.49996427]\n",
      " [0.50002369]\n",
      " [0.4999763 ]\n",
      " [0.50003572]]\n",
      "\n",
      "Loss Akhir: 0.250000\n"
     ]
    }
   ],
   "source": [
    "# AKTIFASI RELU DAN BANDINGKAN HASIL\n",
    "import numpy as np\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size_relu = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Fungsi Aktivasi ReLU dan Sigmoid\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size_relu) * 0.01\n",
    "b1 = np.zeros((1, hidden_size_relu))\n",
    "W2 = np.random.randn(hidden_size_relu, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y - a2\n",
    "    loss = np.mean(np.square(error))\n",
    "\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\nOutput Akhir (Prediksi):\")\n",
    "print(a2)\n",
    "print(f\"\\nLoss Akhir: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e5f92",
   "metadata": {},
   "source": [
    "Pada perbandingan dari percobaan awal, ada perubahan nilai pada output prediksi ke-2 dan ketiga, sedangkan pada hidden_input=3, terlihat bahwa prediksi tertinggi berada pada prediksi ke 3. Sedangkan pada Epoch, hasil array prediksi menunjukkan semua hasil hampir sama pada 0.49 atau 0.5."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
